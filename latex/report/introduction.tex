\section*{Introduction}

This document aims at presenting the work achieved for project Elasto$\Phi$ during the CEMRACS summer school  
that took place at the Centre de Rencontres MathÃ©matiques (CIRM) at Luminy from  July the 24th to 
August the 25th, 2016. This project originated from mathematical challenges encountered by IFP Energies Nouvelles (IFPEN)
for the numerical solution of an elastostatic problem in an infinite homogeneous background medium containing crack 
networks with highly complex geometrical structure, see the pictures below. 

\begin{figure}[h!]
\centerline{\includegraphics[width=0.4\linewidth]{../images/fig1.pdf}}
\end{figure}


\noindent 
This problem was reformulated as boundary integral equation posed at the surface of cracks, and 
discretized by means of Galerkin procedure based on piecewise constant functions, which is commonly known as 
Boundary Element Method (BEM). The fully non-local structure of boundary integral operators leads to  
densly populated matrices. As a consequence if the matrix of the problem is of size $N$, 
then any matrix-vector product (which is the most elementary building block of any iterative linear solver)
requires at least $\mathcal{O}(N^{2})$ operations.

\quad\\
As in most industrial applications, for the problems considered by IFPEN this situation is not acceptable: 
computational complexity of $N^{2}$ makes it impossible to perform a matrix-vector product for $N$ larger than
say $10^{6}$: this would be too costly in terms of time and memory storage. At the same time, the  applications 
considered by IFPEN typically require $N$ to be of this order.

\quad\\
To deal with this difficulty, IFPEN developped  an heuristic method consisting in forcing coefficients of the 
BEM matrix to zero whenever this coefficient corresponds to the interaction between sufficiently distant points 
of the crack network. With this procedure, the matrix of the problem is approximated by a sparsified matrix that 
allows matrix-vector products with $\mathcal{O}(N)$ complexity. But this strategy also induce substantial consistency 
error: measured in Frobenius norm, the perturbation on the matrix is typically  40\% large.

\quad\\
On the other hand, current literature on boundary integral equation nowadays offers a panel of refined complexity reduction 
techniques: fast multipole methods, hierarchical matrix strategies, and the like. These techniques, that have been developped 
during the last two decades, have been introduced to accelerate computations in a wide variety of problems ranging from molecular 
dynamics \cite{?}, to astrophysics \cite{?}. Acceleration of boundary integral equations on smooth surfaces has also been 
historically an key challenge for stimulating the development of such methods \cite{?}.

\quad\\
The main goal of the CEMRACS project Elasto$\Phi$ was to test the performance of one of these reduction techniques on the 
crack network problem of IFPEN. Although the above mentionned complexity reduction techniques are particularly well suited  
for boundary integral equations, the problem under consideration from IFPEN was not just a straightforward application of 
the state of the art, because it involves a strongly irregular geometry. Besides development issues, this was the main 
challenging  aspect of the project.

\quad\\
The Elasto$\Phi$ project mainly conisisted in developping a numerical mockup code in C++ and testing it on the matrices sent 
by IFPEN. We implemented one particular complexity reduction technique: Hierarchical Matrix \cite{?} format combined with the Adaptative Cross 
Approximation (ACA) compression method \cite{?}. For the sake of brevity, we shall refer to this approach as HM-ACA. We could not test several 
complexity reduction techniques, this would have required more time (CEMRACS is only 5 weeks long\dots). Among all complexity reduction techniques 
already available (multipoles, panel clustering, etc\dots), we chose HM-ACA because this was the only approach that treats generation 
of the matrix of the problem in a black-box manner. All other methods are far more intrusive, and most of the time their developement 
is also more time consuming. On the other hand, due to confidentiality restrictions, we did not have access to the source code of IFPEN, and 
in particular could not actually get the routine generating the coefficients of the matrix. These constraints led us to choose HM-ACA.


There already exists freely accessible libraries written in C or C++ that implement HM-ACA, see e.g. HLib (\verb?http://hlib.org/?), H2Lib 
(\verb?http://www.h2lib.org/?) or Ahmed (\verb?https:? \verb?//github.com/xantares/ahmed?),  but they are poorly documented. Besides IFPEN required 
that the code numerical mockup developped during the project can be reused by IFPEN after the CEMRACS summerschool, 
and the licence of all existing libraries was incompatible with this condition. For these reasons, we decided to redevelopped simply  
an "home made" implementation of HM-ACA. The code that we developped was put under Lesser Gnu Public Licence (LGPL) and 
put on a Github repository freely accessible at\\[10pt]
\textcolor{white}{.}\hspace{4cm}\verb?https://github.com/xclaeys/ElastoPhi?\\[10pt]
The outline of this report is as follows. We will first describe the Adptative Cross Approximation method, and show its efficiency 
for the compression of dense matrices admitting fastly dcreasing singular values (such matrices shall be refered to in the sequel as admissible). 
Unfortunately the matrices generated by boundary element  methods (and in particular the matrices considered by IFPEN) do not have 
fastly decreasing eigenvalues. We shall then comment on this point, describing the problem and the boundary integral equation 
considered by IFPEN. Since the ACA compression method is not directly applicable, we are led to decompose the matrix in sub-blocks, 
each of which is either small or admissible. To be efficient, such a decomposition has to follow certain rules, and in particular 
admit a hirerachical structure. Such technique, that we shall then describe, is referred to as a "hierarchical matrix format". 
Then we shall give an detailed overview of the code developped during the project, and conclude the report by a series of test 
cases where we report the performances of our code.

\section{Low rank approximations}

In this section we are interested in fully populated matrices $\mA\in \C^{n\times n}$, $\mA = (\mA_{j,k})_{j,k=1\dots n}$ with, a priori, 
none of its entries vanishing, its size $n$ being potentially large. With no particular assumption on this matrix, the cost of a 
matrix-vector product is $\mathcal{O}(n^{2})$. 

\quad\\
This cost is substantially reduced if we assume that $\mA$ is of low rank though. We say that a matrix has the low rank property, 
with rank $k\leq n$, if there exist vectors $\bfu_{j},\bfv_{j}\in \C^{n}, j=1\dots k$, such that 
$$
\mA = \sum_{j=1}^{k}\bfu_{j}\cdot\bfv_{j}^{T}\quad \textrm{with}\quad k\leq n/2.
$$ 
Indeed if this representation holds, then a matrix-vector product requires $2 k n$ flops, which is smaller than $n^{2}$ provided 
that the condition on $k$ given above is satisfied. Matrices $\mA$ encountered in applications rarely have the loaw rank property. 
A simple primary observation is that general matrices $\mA$ can as a sum rank one matrices through its singular value
decomposition
\begin{equation}\label{SVD}
\mA = \sum_{j=1}^{n}\sigma_{j}\,\bfu_{j}\cdot\bfv_{j}^{T}\quad \textrm{where}\;\; \mathfrak{S}(\mA^{*}\mA) = \{\sigma_{j}^{2}\}_{j=1\dots n}
\end{equation}
where $(\bfu_{j})_{j=1}^{n}, (\bfv_{j})_{j=1}^{n}$ are orthonormal basis of $\C^{n}$ and $\sigma_{1}\geq \sigma_{2}\geq \dots \geq \sigma_{n}$. 
A closer inspection of this formula leads to the conclusion, provided that the sequence $(\sigma_{j})$  decreases fast, then 
truncating the singular value decomposition (\ref{SVD}) yields a good approximation of the matrix $\mA$. This is the essence of 
the next result.

\begin{proposition}\quad\\
Let $\mA\in \C^{n\times n}$ admit the singular value decomposition (\ref{SVD}), and denote $\mA_{k}$ the 
matrix obtained by truncating this decomposition at rank $k$ namely $\mA_{k}:=\sum_{j=1}^{k}\sigma_{j}\,\bfu_{j}
\cdot\bfv_{j}^{T}$. Then we have the error estimates
$$
\Vert \mA - \mA_{k}\Vert_{2}^{2} = \sigma_{k+1}^{2}
\quad\textrm{and}\quad 
\Vert \mA - \mA_{k}\Vert_{\mrm{F}}^{2} = \sum_{j=k+1}^{n}\sigma_{j}^{2}
$$
where $\Vert \;\Vert_{2}$ refers to refers to the matrix norm induced by the vector norm $\vert \bfu\vert_{2} = (\sum_{j=1}^{n}\vert u_{j}\vert^{2})$ 
for $\bfu = (u_{j})_{j=1}^{n}\in \C$ and $\Vert \;\Vert_{\mrm{F}}$ refers to the Fr\"obenius norm.
\end{proposition}

\noindent 
Truncating the SVD is thus an efficient way to approximate a matrix, and thus to reduce the cost of the 
matrix-vector product, provided that  the singular values decrease fast. Assume that singular values decrease 
exponentially, say $\sigma_{k}\leq q^{-k}$ for a fixed $q\in (0,1)$. Then for a relative error expressed in Fr\"obenius norm of order $\epsilon>0$,  
it suffices to take $k \simeq \vert\ln \epsilon\vert$. 

\quad\\
Unfortunately, computing the singular value 
decomposition  of a matrix is expensive: it costs $\mathcal{O}(n^{3})$ flops. To circumvent this issue, starting from 
an arbitrary matrix $\mA$ whose singular values are assumed to decrease exponentially, the Adaptative 
Cross Approximation algorithm provides a collection of vectors $\bfu_{j},\bfv_{j}\in \C^{n}, j=1\dots n$ such that 
the matrices $\widetilde{\mA}_{k} = \sum_{j=1}^{k}\bfu_{j}\cdot\bfv_{j}^{T}$ satisfy 
$$
\Vert \mA - \widetilde{\mA}_{k}\Vert_{\mrm{F}}\leq C \Vert \mA - \mA_{k}\Vert_{\mrm{F}}
$$
for some constant $C>0$ independent of $k$. Moreover, which is probably the most interesting feature of this method, 
the cost of computing $\widetilde{\mA}_{k}$ is $\mathcal{O}(kn)$. This cost is thus quasi-linear provided that the singular 
values  decrease exponentially. The detailed analysis of the ACA algorithm is out of the scope of this report. We only 
give the algorithm it self.

\begin{algorithm}
  \caption{Partially Pivoted ACA}
  Initialise $j_{*}$\\
  $r=0$\\
  \textbf{while}(stopping criterion not satisfied)\{\\\quad\\
  \indent\hspace{0.5cm} \parbox{\linewidth}{
    $\bfw = \mA(j_{*},:)^{T} - \sum_{\ell=1}^{r}\bfu_{\ell}(j_{*})\,\bfv_{\ell}$\\
    $k_{*} = \mathop{\mrm{argmax}}_{k = 1\dots n}\vert \bfw(k)\vert$\\
    $w_{*} = \bfw(k_{*})$\\\quad\\
    \textbf{if}($w_{*}\neq 0$)\{\\\quad\\
    \indent\hspace{0.5cm} \parbox{\linewidth}{
      $\bfv_{r+1} = \bfw$\\
      $\bfw = \mA(:,k_{*})-\sum_{\ell = 1}^{r}\bfv_{\ell}(k_{*})\,\bfu_{\ell} $\\
      $\bfu_{r+1} = w_{*}^{-1}\bfw$\\      
      $j_{*} = \mathop{\mrm{argmax}}_{j = 1\dots n} \vert \bfw(j)\vert$\\
      $r=r+1$
    }
    \quad\\
    \}\\\quad\\
    \textbf{else}\{terminate algorithm\}
    }\\
  \}
\end{algorithm}







 